{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkvU-ociK6HW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4379daa2-29a7-44e5-b70d-ec52c726c7d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CLIP'...\n",
            "remote: Enumerating objects: 247, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 247 (delta 0), reused 1 (delta 0), pack-reused 243\u001b[K\n",
            "Receiving objects: 100% (247/247), 8.93 MiB | 16.47 MiB/s, done.\n",
            "Resolving deltas: 100% (124/124), done.\n",
            "Cloning into 'taming-transformers'...\n",
            "remote: Enumerating objects: 1342, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 1342 (delta 0), reused 1 (delta 0), pack-reused 1340\u001b[K\n",
            "Receiving objects: 100% (1342/1342), 409.77 MiB | 25.34 MiB/s, done.\n",
            "Resolving deltas: 100% (282/282), done.\n"
          ]
        }
      ],
      "source": [
        "##\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "#https://github.com/openai/CLIP\n",
        "#CLIP (Contrastive Language-Image Pre-Training)\n",
        "#Learning Transferable Visual Models From Natural Language Supervision\n",
        "#Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\n",
        "#Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever\n",
        "\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "#https://github.com/CompVis/taming-transformers\n",
        "#Taming Transformers for High-Resolution Image Synthesis\n",
        "#Patrick Esser, Robin Rombach, Bj√∂rn Ommer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##figure out what libraries to install"
      ],
      "metadata": {
        "id": "L79bbWxOf2Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##import libraries"
      ],
      "metadata": {
        "id": "fHzfvaCpf_Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##helper functions\n",
        "\n",
        "def show_from_tensor(tensor):\n",
        "  img = tensor.clone()\n",
        "  img = img.mul(255).byte()\n",
        "  img = img.cpu().numpy().transpose((1,2,0))\n",
        "\n",
        "  plt.figure(figsize=(10,7))\n",
        "  plt.axis('off')\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "\n",
        "def norm_data(data):\n",
        "  return (data.clip(-1,1)+1)/2 ### range between 0 and 1 in the result\n",
        "\n",
        "### Parameters\n",
        "learning_rate = .5\n",
        "batch_size = 1\n",
        "wd = .1\n",
        "noise_factor = .22\n",
        "\n",
        "total_iter=400\n",
        "im_shape = [450, 450, 3] # height, width, channel\n",
        "size1, size2, channels = im_shape\n"
      ],
      "metadata": {
        "id": "yQTJvoUHgCR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### CLIP MODEL ###\n",
        "clipmodel, _ = clip.load('ViT-B/32', jit=False)\n",
        "clipmodel.eval()\n",
        "print(clip.available_models())\n",
        "\n",
        "print(\"Clip model visual input resolution: \", clipmodel.visual.input_resolution)\n",
        "\n",
        "device=torch.device(\"cuda:0\")\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "VuVb1OYeSv2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##What transformers do we wanna use??"
      ],
      "metadata": {
        "id": "S4P_SUOUQWbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_y-3G2wHQWkv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}